---
layout: post
title: Making decisions with data
location: New York, NY, USA
excerpt: Many companies don’t know if they are profitable or not until they run the end-of-the-quarter reports. For months before that they try to stay under the budget and hope things work out. This is the norm created by the lack of tools to crunch the records in real time. But as storage and computing is becoming cheaper, the delay time is reducing. The concept of sitting down to “run the reports” is disappearing.
---

Many companies don’t know if they are profitable or not until they run the end-of-the-quarter reports. For months before that they try to stay under the budget and hope things work out. This is the norm created by the lack of tools to crunch the records in real time. But as storage and computing is becoming cheaper, the delay time is reducing. The concept of sitting down to “run the reports” is disappearing.

The challenge is how to integrate this into the general process. Instead of producing reports with tables and charts, which is a great first step nonetheless, the goal is to create systems which could react automatically. [Mint](http://www.mint.com) for example makes it easy to analyze one’s spending, but it won’t do anything about it. It is a passive tool. [Digit](https://digit.co/) on the other hand monitors spending and saves money by itself. Both of them *could* help most people save money, but they try to achieve it in different ways.

Recently health tracking apps got popular, so much that sleep, steps walked, calories eaten, and a lot more can all be tracked.  This comes from the mindset that more data is always better than less, and by tracking every metric some insights will come out. Some say [More data usually beats better algorithms](http://anand.typepad.com/datawocky/2008/03/more-data-usual.html). The whole “Big Data” trend for anyone not at the size of Facebook is nothing more than a blind race to collect gigabytes. I’d argue that the focus instead should be on computable insights. The amount of data involved is largely irrelevant. Not having to store and process enormous amounts of excess data is cheaper, faster and much easier to implement.

Of course any process needs to reach a certain size before it can be analyzed. Depending on the context it might mean that the company or an app is simply too small to benefit. It would only cause confusion and errors. If an ecommerce site is selling a few dozen products a day, it’s practically impossible to use that to make any sort of prediction. There is too much variance and thus the data is very noisy. As [The Joyless World of Data-Driven Startups](https://medium.com/backchannel/the-joyless-world-of-data-driven-startups-b6f475f11f5f) puts it:

> As counter-intuitive as it sounds, I hypothesize that an early startup guided primarily by gut decisions from a strong strategic vision will be more cohesive and deliver a stronger offering than a startup created from a random walk of data-driven decisions. Though I don’t have the data to back up my claim.

Amazon AWS cloud is a dream for the data savvy. Everything is measured and graphed. But what if instead of it saying “*You are currently running 1 environment with a total of 4 instances who, every day, generate approximately 10GB of incoming traffic and 50GB of outgoing traffic*” it would simply state “*You have 4 running instances who’ve had little traffic for the last few days, do you really need them all? We suggest stopping 2 instances*.”. Or better yet, it would drop an email that it looked at the stats and stopped those instances by itself. Yet again, one provides a lot of relevant data, but in the end of the day, the end result is the same. Just do it, don’t make me think about it.

However an important part of the puzzle is domain knowledge. If on one side you have the “*business people*”, who understand the domain, but can’t program. And on the other side the developers, who might not know why things are the way they are. No good data-driven systems will get built. Someone has to be in the middle, and be able to do both of these things. Not only that, but they also need to know the core principles of why and how predictions work. Which most people don’t and thus it’s hard for them to reason about what could be analyzed and how that would be useful.

The one system everyone knows is a lie detector. Used in many movies, most people realize that it could notify when someone is lying. But lie detectors can’t tell if someone is lying. They only allow to exhibit a change in physiological behavior, so a trained operator could estimate the chance of it being a lie. The fundamental misconception is that a lie detector gives a yes or no answer. It doesn’t. What it shows is a lot of data, which an expert can use to make the final call.

Out of many great examples in [The Signal and the Noise: Why So Many Predictions Fail](http://www.amazon.com/The-Signal-Noise-Many-Predictions/dp/159420411X) by Nate Silver, the one I like the most is about weather predictions. Majority of the people think they are unreliable and are always wrong. Not only have they’ve been improving year-over-year, but we figured out the importance of communicating forecasts. A weather forecast, like a lie detector, produces a lot of data. But since almost everyone are bad at interpreting statements like “75% chance of 1 inch of snow, 50% chance of 2-3 inches of snow”, instead the forecast is tailored to the context. For busy cities like New York, it’s better to say that it’s going to snow and it wouldn’t, than have millions of people annoyed by an unexpected snowfall.

Increase in interest in data has spurred many generalized tools. As if as simply as by downloading an app the company could transform into a data-driven one. I agree with [Surviving Data Science “at the Speed of Hype”](http://www.john-foreman.com/blog/surviving-data-science-at-the-speed-of-hype), this is not a technical problem, the tools are there and are good enough:

> A lot of vendors want to cast the problem as a technological one. That if only you had the right tools then your analytics could stay ahead of the changing business in time for your data to inform the change rather than lag behind it.
>
> This is bullshit.

Finding the data is key. As much as a lot of companies would like to know if they are making money or not in real-time, they can’t because the data doesn’t exist. Or it exists in 5 different systems, a few stacks of papers and depending on outside sources to supply it. In that case, no fancy tool will solve the problem. It requires organization changes and good practices to make sure everything can be accessed.

The step where the system executes the decision is scary. If the software has bugs it can make products fail, companies go bankrupt or cause a lot of embarrassment. Thus before the system can be trusted to operate without supervision, it should provide *smart* reports. Not the usual full-screen tables in reports, but some actionable items like in the Amazon AWS example. The ideal model is to find some areas which look like they could be analyzed, raise some hypothesis and see if it works.

If we have data, let’s look at data. If all we have are opinions, let’s find data.
